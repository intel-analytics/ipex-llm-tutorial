{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Langchain Integrations \n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a popular library for developing applications powered by language models. You can use LangChain with LLMs to build various interesting applications such as [Chatbot](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/langchain/transformers_int4/chat.py), [Document Q&A](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/langchain/transformers_int4/docqa.py), [voice assistant](https://github.com/intel-analytics/BigDL/blob/main/python/llm/example/langchain/transformers_int4/voiceassistant.py). BigDL-LLM provides LangChain integrations (i.e. LLM wrappers and embeddings) and you can use them the same way as [other LLM wrappers in LangChain](https://python.langchain.com/docs/integrations/llms/). \n",
    "\n",
    "This notebook goes over how to use langchain to interact with BigDL-LLM.\n",
    "\n",
    "## 5.1 Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, install BigDL-LLM in your prepared environment. For best practices of environment setup, refer to [Chapter 2]() in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then install LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.248"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 LLM Wrapper\n",
    "\n",
    "BigDL-LLM provides `TransformersLLM` and `TransformersPipelineLLM`, which implement the standard interface of LLM wrapper of LangChain.\n",
    "\n",
    "`TransformerLLM` can be instantiated using `TransformerLLM.from_model_id` from a huggingface model_id or path. Model generation related parameters (e.g. `temperature`, `max_length`) can be passed in as a dictionary in `model_kwargs`. Let's use `vicuna-7b-v1.5` model as an example to instatiate `TransformerLLM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.langchain.llms import TransformersLLM\n",
    "\n",
    "llm = TransformersLLM.from_model_id(\n",
    "        model_id=\"lmsys/vicuna-7b-v1.5\",\n",
    "        model_kwargs={\"temperature\": 0, \"max_length\": 1024, \"trust_remote_code\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> `TransformersPipelineLLM` can be instantiated in similar way as `TransformersLLM` from a huggingface model_id or path, `model_kwargs` and `pipeline_kwargs`. Besides, there's an extra `task` parameter which specifies the type of task to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a prompte template to format the prompt and simply call `llm` to test generation.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> `max_new_tokens` parameter defines the maximum number of tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI stands for \"Artificial Intelligence.\" It refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI can be achieved through a combination of techniques such as machine learning, natural language processing, computer vision, and robotics. The ultimate goal of AI research is to create machines that can think and learn like humans, and can even exceed human capabilities in certain areas.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is AI?\"\n",
    "VICUNA_PROMPT_TEMPLATE = \"USER: {prompt}\\nASSISTANT:\"\n",
    "result = llm(prompt=VICUNA_PROMPT_TEMPLATE.format(prompt=prompt), max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `generate` on LLM to get batch results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([VICUNA_PROMPT_TEMPLATE.format(prompt=\"Tell me a joke\"), VICUNA_PROMPT_TEMPLATE.format(prompt=\"Tell me a poem\")]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------number of generations--------------------\n",
      "6\n",
      "--------------------the first generation--------------------\n",
      "USER: Tell me a joke\n",
      "ASSISTANT: Why did the tomato turn red?\n",
      "\n",
      "Because it saw the salad dressing!\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*20+\"number of generations\"+\"-\"*20)\n",
    "print(len(llm_result.generations))\n",
    "print(\"-\"*20+\"the first generation\"+\"-\"*20)\n",
    "print(llm_result.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Using Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's begin using LLM wrappers and embeddings in [Chains](https://docs.langchain.com/docs/components/chains/).\n",
    "\n",
    ">**Note**\n",
    "> Chain is an important component in LangChain, which combines a sequence of modular components (even other chains) to achieve a particular purpose. The compoents in chain may be propmt templates, models, memory buffers, etc. \n",
    "\n",
    "### 5.4.1 LLMChain\n",
    "\n",
    "Let's first try use a simple chain `LLMChain`. \n",
    "\n",
    "Create a simple prompt template as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template =\"USER: {question}\\nASSISTANT:\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `llm` we created in previous section and the prompt tempate we just created to instantiate a `LLMChain`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask the llm a question and get the response by calling `run` on `LLMChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI stands for \"Artificial Intelligence.\" It refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI can be achieved through a combination of techniques such as machine learning, natural language processing, computer vision, and robotics. The ultimate goal of AI research is to create machines that can think and learn like humans, and can even exceed human capabilities in certain areas.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is AI?\"\n",
    "result = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Conversation Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a chat application, we can use a more complex chain with memory buffers to remember the chat history. This is useful to enable multi-turn chat experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"The following is a friendly conversation between a human and an AI.\\\n",
    "    The AI is talkative and provides lots of specific details from its context.\\\n",
    "    If the AI does not know the answer to a question, it truthfully says it does not know.\\\n",
    "    \\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI Asistant:\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"history\", \"input\"])\n",
    "conversation_chain = ConversationChain(\n",
    "    verbose=True,\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),\n",
    "    llm_kwargs={\"max_new_tokens\": 256},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI.    The AI is talkative and provides lots of specific details from its context.    If the AI does not know the answer to a question, it truthfully says it does not know.    \n",
      "Current conversation:\n",
      "\n",
      "Human: Good morning AI!\n",
      "AI Asistant:\u001b[0m\n",
      "Good morning! How can I assist you today?\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query =\"Good morning AI!\" \n",
    "result = conversation_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI.    The AI is talkative and provides lots of specific details from its context.    If the AI does not know the answer to a question, it truthfully says it does not know.    \n",
      "Current conversation:\n",
      "Human: Good morning AI!\n",
      "AI: The following is a friendly conversation between a human and an AI.    The AI is talkative and provides lots of specific details from its context.    If the AI does not know the answer to a question, it truthfully says it does not know.    \n",
      "Current conversation:\n",
      "\n",
      "Human: Good morning AI!\n",
      "AI Asistant: Good morning! How can I assist you today?\n",
      "Human: Tell me about Intel.\n",
      "AI Asistant:\u001b[0m\n",
      "Intel is a multinational technology company that specializes in the development and manufacturing of computer processors and related technologies. It was founded in 1976 by Robert Noyce and Gordon Moore, and is headquartered in Santa Clara, California. Intel's processors are used in a wide range of devices, including personal computers, servers, smartphones, and other electronic devices. The company is also involved in the development of new technologies, such as artificial intelligence and autonomous driving.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conversation_chain.memory.clear()\n",
    "query =\"Tell me about Intel.\" \n",
    "result = conversation_chain.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 MathChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try use LLM solve some math problem, using `MathChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** \n",
    "> MathChain usually need LLMs to be instantiated with larger `max_length`, e.g. 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "MATH_CHAIN_TEMPLATE =\"Question: {question}\\nAnswer:\"\n",
    "prompt = PromptTemplate(template=MATH_CHAIN_TEMPLATE, input_variables=[\"question\"])\n",
    "llm_math = LLMMathChain.from_llm(prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the 2 power"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk3/miniconda3/envs/cn-eval/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 raised to the 2 power is equal to 13 \\* 13, which is 169.\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is 13 raised to the 2 power\n",
      "Answer: 13 raised to the 2 power is equal to 13 \\* 13, which is 169.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer:  13 raised to the 2 power is equal to 13 \\\\* 13, which is 169.'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is 13 raised to the 2 power\"\n",
    "llm_math.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Question Answering over Docs\n",
    "Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents. LLMs, given their proficiency in understanding text, are a great tool for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==0.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.1 Load document\n",
    "For convienence, here we use a text string as a loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_doc = \"\\\n",
    "    BigDL: fast, distributed, secure AI for Big Data\\\n",
    "    BigDL seamlessly scales your data analytics & AI applications from laptop to cloud, with the following libraries:\\\n",
    "        Orca: Distributed Big Data & AI (TF & PyTorch) Pipeline on Spark and Ray\\\n",
    "        Nano: Transparent Acceleration of Tensorflow & PyTorch Programs on XPU\\\n",
    "        DLlib: “Equivalent of Spark MLlib” for Deep Learning\\\n",
    "        Chronos: Scalable Time Series Analysis using AutoML\\\n",
    "        Friesian: End-to-End Recommendation Systems\\\n",
    "        PPML: Secure Big Data and AI (with SGX Hardware Security)\\\n",
    "        LLM: A library for running large language models with very low latency using low-precision techniques on Intel platforms\\\n",
    "    \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2 Split texts of input document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(input_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.3 Create embeddings and store into vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigDL-LLM also provides `TransformersEmbeddings`, which allows you to obtain embeddings from text input using LLM.\n",
    "\n",
    "`TransformersEmbeddings` can be instantiated the similar way as `TransformersLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.langchain.embeddings import TransformersEmbeddings\n",
    "\n",
    "embeddings = TransformersEmbeddings.from_model_id(model_id=\"lmsys/vicuna-7b-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the embeddings by `embed_query`, and `embed_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------length of query embedding--------------------\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*20+\"length of query embedding\"+\"-\"*20)\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing `TransformersEmbeddings`, let's create embeddings and store into vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from bigdl.llm.langchain.embeddings import TransformersEmbeddings\n",
    "\n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.4 Get relavant texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------number of relevant documents--------------------\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "query = \"What is BigDL?\"\n",
    "docs = docsearch.get_relevant_documents(query)\n",
    "print(\"-\"*20+\"number of relevant documents\"+\"-\"*20)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.5 Prepare chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.chat_vector_db.prompts import QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "bigdl_llm = TransformersLLM.from_model_id(\n",
    "    model_id=model_path,\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 1024, \"trust_remote_code\": True},\n",
    ")\n",
    "\n",
    "doc_chain = load_qa_chain(\n",
    "    bigdl_llm, chain_type=\"stuff\", prompt=QA_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.6 Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigDL is a fast, distributed, secure AI library for Big Data. It provides a range of libraries for data analytics and AI applications, including Orca, Nano, DLlib, Chronos, Friesian, PPML, and LLM.\n"
     ]
    }
   ],
   "source": [
    "result = doc_chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
