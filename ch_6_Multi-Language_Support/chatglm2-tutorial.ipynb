{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6.1: ChatGLM2\n",
    "\n",
    "## 6.1.1 Overview\n",
    "This is an example shows how to run [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) Chinese inference on low-cost PCs (without the need of discrete GPU) using [BigDL-LLM](https://github.com/intel-analytics/BigDL/tree/main/python/llm) APIs. ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) proposed by [THUDM](https://github.com/THUDM). ChatGLM2-6B also can be found in [Huggingface models](https://huggingface.co/models) in following [link](https://huggingface.co/THUDM/chatglm2-6b).\n",
    "\n",
    "Before conducting inference, you may need to prepare environment according to [Chapter 2 Environment Setup](../ch_2_Environment_Setup/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Installation\n",
    "\n",
    "Install BigDL-LLM through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bigdl-llm[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The all option is for installing other required packages by BigDL-LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.3 Load Model and Tokenizer\n",
    "\n",
    "### 6.1.3.1 Load Model\n",
    "\n",
    "Load model with low-precision optimization(INT4) for lower resource cost using BigDL-LLM APIs, which convert the relevant layers in the model into INT4 format. You can specify the argument `model_path` with both Huggingface repo id or local model path.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> BigDL-LLM has supported `AutoModel`, `AutoModelForCausalLM`, `AutoModelForSpeechSeq2Seq` and `AutoModelForSeq2SeqLM`. The AutoClasses help users automatically retrieve the relevant model, in this case, we can simply use `AutoModel` to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModel\n",
    "\n",
    "model_path = \"THUDM/chatglm2-6b\" # repo id or model path\n",
    "model = AutoModel.from_pretrained(model_path,\n",
    "                                  load_in_4bit=True,\n",
    "                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3.2 Load Tokenizer\n",
    "\n",
    "The quantized model is compatible with the tokenizer provided by the [Huggingface transformers library](https://huggingface.co/docs/transformers/index). Thus, you can directly use Huggingface transformers APIs to load tokenizer. A tokenizer maps between texts and lists of integers. We use it to encode input texts to integers for model to calculate, and decode the model output integers to texts for human to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.4 Inference\n",
    "\n",
    "### 6.1.4.1 Create Prompt Template\n",
    "\n",
    "Before inference, you need to create a prompt template. Here we give an example prompt template refers to [ChatGLM2-6B prompt template](https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1007). You can tune the prompt based on your own model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATGLM_V2_PROMPT_TEMPLATE = \"问：{prompt}\\n\\n答：\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4.2 Generate\n",
    "\n",
    "Then, you can generate output with loaded model and tokenizer.\n",
    "\n",
    "> **Note**\n",
    "> \n",
    "> `max_new_tokens` parameter in the `generate` function defines the maximum number of tokens to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: xxxx s\n",
      "-------------------- Output --------------------\n",
      "问：AI是什么？\n",
      "\n",
      "答： AI指的是人工智能,是一种能够通过学习和推理来执行任务的计算机程序。它可以模仿人类的思维方式,做出类似人类的决策,并且具有自主学习、自我进化的能力。AI在许多领域都具有广泛的应用,例如自然语言处理、计算机视觉、机器学习、深度学习等。\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "prompt = \"AI是什么？\"\n",
    "n_predict = 128\n",
    "\n",
    "with torch.inference_mode():\n",
    "    prompt = CHATGLM_V2_PROMPT_TEMPLATE.format(prompt=prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    st = time.time()\n",
    "    output = model.generate(input_ids,\n",
    "                            max_new_tokens=n_predict)\n",
    "    end = time.time()\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f'Inference time: {end-st} s')\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4.3 Stream Chat\n",
    "\n",
    "ChatGLM2-6B support streaming output function `stream_chat`, which enable the model to provide a streaming response word by word. However, other models may not provide similar APIs, if you want to implement general streaming output function, please refer to [Chapter 4.1](../ch_4_Run_Models/4_1_Run_Transformer_Models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Stream Chat Output --------------------\n",
      "AI指的是人工智能,agogue是一种能够通过学习和理解数据来执行任务的计算机程序。它可以模仿人类的思维方式、学习、推理、理解语言,甚至可以进行一些需要人类直觉和判断的任务。\n",
      "\n",
      "AI 技术包括机器学习、深度学习、自然语言处理、计算机视觉等,可以应用于各种领域,如医疗、金融、零售、能源、交通等。\n",
      "\n",
      "AI绿豆是一种具有自主学习、自我进化的能力的人工智能系统,能够通过大量的数据和算法训练,逐渐提高自身的准确度和智能化程度,并且可以模拟出人类类似思维模式来处理各种问题。猖獗AI是一种能够模拟出人类智能的计算机程序,可以进行学习、思考、推理、决策等复杂的任务,并且可以应用于各种领域,如游戏、金融、医疗等。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    question = \"AI 是什么？\"\n",
    "    response_ = \"\"\n",
    "    print('-'*20, 'Stream Chat Output', '-'*20)\n",
    "    for response, history in model.stream_chat(tokenizer, question, history=[]):\n",
    "        print(response.replace(response_, \"\"), end=\"\")\n",
    "        response_ = response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.5 Use in LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a widely used framework for developing applications powered by language models. In this section, we will show how to integrate BigDL-LLM with LangChain. You can follow this [instruction](https://python.langchain.com/docs/get_started/installation) to prepare environment for LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need, install LangChain through, or you can refer to [Chapter 5 Langchain Integrations](../ch_5_Langchain_Integrations/README.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.184\n",
    "!pip install -U typing_extensions==4.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5.1 Create Prompt Template\n",
    "\n",
    "Before inference, you need to create a prompt template. Here we give an example prompt template contains two input variables, `history` and `human_input`. You can tune the prompt based on your own model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATGLM_V2_LANGCHAIN_PROMPT_TEMPLATE = \"\"\"{history}\\n\\n问：{human_input}\\n\\n答：\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5.2 Prepare Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [LangChain API](https://api.python.langchain.com/en/latest/api_reference.html) `LLMChain` to construct a chain for inference. Here we use BigDL-LLM APIs to construct a `LLM` object, which will load model with low-precision optimization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from bigdl.llm.langchain.llms import TransformersLLM\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "llm_model_path = \"THUDM/chatglm2-6b\" # the path to the huggingface llm model\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=CHATGLM_V2_LANGCHAIN_PROMPT_TEMPLATE)\n",
    "max_new_tokens = 128\n",
    "\n",
    "llm = TransformersLLM.from_model_id(\n",
    "        model_id=llm_model_path,\n",
    "        model_kwargs={\"trust_remote_code\": True},\n",
    ")\n",
    "\n",
    "# Following code are complete the same as the use-case\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    llm_kwargs={\"max_new_tokens\":max_new_tokens},\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5.3 Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "问：AI 是什么？\n",
      "\n",
      "答：\u001b[0m\n",
      "AI指的是人工智能,是一种能够通过学习和理解数据,以及应用数学、逻辑、推理等知识,来实现与人类智能相似或超越人类智能的计算机系统。AI可以分为弱人工智能和强人工智能。弱人工智能是指一种只能完成特定任务的AI系统,比如语音识别或图像识别等;而强人工智能则是一种具有与人类智能相同或超越人类智能的AI系统,可以像人类一样思考、学习和理解世界。目前,AI的应用领域已经涵盖了诸如自然语言处理、计算机视觉、机器学习、深度学习、自动驾驶、医疗健康等多个领域。\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "问：AI 是什么？\n",
      "\n",
      "答： AI指的是人工智能,是一种能够通过学习和理解数据,以及应用数学、逻辑、推理等知识,来实现与人类智能相似或超越人类智能的计算机系统。AI可以分为弱人工智能和强人工智能。弱人工智能是指一种只能完成特定任务的AI系统,比如语音识别或图像识别等;而强人工智能则是一种具有与人类智能相同或超越人类智能的AI系统,可以像人类一样思考、学习和理解世界。目前,AI的应用领域已经涵盖了诸如自然语言处理、计算机视觉、机器学习、深度学习、自动驾驶、医疗健康等多个领域。\n"
     ]
    }
   ],
   "source": [
    "text = \"AI 是什么？\"\n",
    "response_text = llm_chain.run(human_input=text,stop=\"\\n\\n\")\n",
    "print(response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cn-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
